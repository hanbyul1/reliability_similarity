%===========================================================
\subsubsection{Similarity in FODS}
\label{sssec:fods-similarity}
%===========================================================
Figure~\ref{fig:fods-similarities} presents the similarity tables for the FODS by activities, where the highest values are denoted in green and the lowest in orange. 

\begin{itemize}
	\item Table (a) shows the similarity for activity P1, comparing the requirements produced by the models. Initially, the similarity of functional and non-functional requirements was analyzed separately and then combined. The table reveals that Meta aligns most closely with other models, achieving an average similarity score of 4, while Gemini exhibits the greatest difference with an average similarity of 3.25. Meta's high similarity is attributed to its higher functional similarity (4.25, not shown in the table), and Gemini's lower score results from its lesser functional similarity (3.5, also not shown). The SMA for P1 is computed at 3.6.
	
	\item Table (b) covers activity P2 where use cases are compared. It shows Meta as the most similar to other models with an average of 3.5, while ChatGPT and Copilot have the lowest similarity scores, each at 3. Meta's high score, which includes coverage of 14 use cases, is attributed to its strong alignment with ChatGPT and Claude, which cover a similar number of use cases --14 and 16, respectively. The low similarity of ChatGPT is particularly due to its minimal similarity with Claude, scoring only 2.
	
	\item Table (c) presents the similarity for activity P3, which is evaluated based on the contents of use case specifications, including primary actor, pre- and post-conditions, main success scenario, and alternative scenarios. For this analysis, we specifically focused on the ``Place Order'' use case, instead of examining all use cases, due to the varying coverage provided by each model. Some models produced specifications for all identified use cases, while others produced only some with annotations suggesting that other specifications could be developed similarly. Therefore, we selected ``Place Order'' as it was common to all models. Meta scores the lowest at 2.24 due to generally lower similarity across other models, while Claude scores the highest at 3.25, consistently showing higher alignment with other models. The SMA for this activity is relatively low at 2.9, reflecting the models' distinct approaches in detailing use case specifications.
	
	\item Table (d) details the similarity for activity P4, which compares domain classes. Meta exhibits the highest similarity with a score of 3.5, while ChatGPT records the lowest at 2.75. This finding is notable because ChatGPT, covering the most classes with a total of 15, shows minimal overlap with other models, suggesting a unique context. Conversely, Meta, covering only 8 classes, has a high overlap with other models, indicating a more general context. The SMA for this activity is calculated to be 3.1.
	
	\item Taking a similar approach to Table (d) for P2, Table (e) displays the similarity for activity P5, which involves comparing system operations specific to the ``Place Order'' use case. It shows that Gemini notably has the highest similarity with a score of 3.25, while Claude and Copilot both have the lowest at 1.75. The considerable variability in similarity can be attributed to the significant differences in the number of operations generated by each model -- ChatGPT with 24, Claude with 46, Copilot with 8, Gemini with 27, and Meta with 23 -- highlighting the distinct approaches individual models took in identifying system operations. The SMA for P5 is recorded at 2.4.
	
	\item Table (f) focuses on P6, comparing the ``Place Order'' sequence diagram, which is common to all models, in terms of participants and sequence of operations. Copilot scores the highest at 3.25, significantly aligning with ChatGPT, while Meta is the lowest at 2.5 due to lesser similarity with ChatGPT and Gemini. The SMA for P6 is 2.9.
	
	\item Table (g) for P7, which compares design classes, shows Meta as the highest at 3, while other models score lower at 2.5. The SMA for P7 is 2.6. Compared to the similarity score of domain class diagrams in P4, which is 3.1, the similarity for design class diagrams is lower, suggesting that models incorporate more of their unique domain knowledge when developing specific design solutions.
	
	\item Table (h) details the similarity for activity P8, comparing attributes and operations of the ``Order'' class, a common class to all models. Claude and Meta exhibit higher similarity with scores of 4.25, while ChatGPT and Gemini are lower at 3.75. The SMA for P8 is calculated at 4. This high SMA is primarily attributed to the models focusing more on attributes, which demonstrate high similarity across the board, rather than on operations. Among the models, only three produced operations -- ChatGPT with 3, Claude with 11, and Meta with 2 -- while Copilot and Gemini did not generate any operations, which had minimal contribution to the SMA. 
	
	\item Finally, Table (i) for P9 compares system tests on the ``Place Order'' scenario, which is common to all models, in terms of involved methods. ChatGPT scores relatively higher at 3, while Claude and Meta are lower at 2.5. The SMA for P9 is 2.7, reflecting the unique approaches of models in this scenario, consistent with P5, P6, and P7.
\end{itemize}