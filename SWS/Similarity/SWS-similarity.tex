%===========================================================
\subsubsection{\textcolor{red}{Similarity in SWS}}
\label{sssec:sws-similarity}
%===========================================================
\textcolor{red}{Figure~\ref{fig:sws-similarities} presents the similarity tables for the SWS.} 

\begin{itemize}
	\item \textcolor{red}{Table (a) presents the similarity for activity P1. It shows that overall, models are very similar in identifying requirements with an average similarity score of 4. Copilot aligns most closely with other models, achieving an average similarity score of 4.5, while Claude shows the greatest deviation with an average similarity of 3.25. Copilot's high similarity is due to its strict adherence to the system description when identifying requirements, which are also covered in other models. Conversely, Claude's lower score stems from its extensive interpretation of the system description in identifying both functional and non-functional requirements using its domain knowledge, which includes exclusive requirements (e.g., {\em support programmable transaction}, {\em verify transaction authenticity}) not covered by other models.}
	
	\item \textcolor{red}{Table (b) displays the similarity for activity P2, revealing that models are similar in identifying use cases from requirements with an average score of 3.5. This lower similarity score compared to the 4.0 in P1 can be attributed to each model's deviation from the identified requirements in P1, influenced by their domain knowledge when identifying use cases. Copilot's high score of 4.0 is due to its strict adherence to the requirements identified in P1, which closely align with the system description used in identifying use cases and are similarly covered by other models. Conversely, Meta's lower score results from its inclusion of unique use cases such as \emph{Set Up Smart Wallet} and \emph{Deposit Cryptocurrency}, which are not covered by other models.}
	
	\item \textcolor{red}{Table (c) shows the similarity for activity P3, indicating a lower similarity among models with an average score of 2.9 in developing use case specifications. In this analysis, we specifically focused on the ``Transfer Assets'' use case, which was common to all models. Continuing the trend of high similarity observed in P1 and P2, Copilot again demonstrated the highest similarity in developing use case specifications for reasons similar to those in P1 and P2. Conversely, ChatGPT recorded the lowest score of 2.5 due to its emphasis on preparing for fund transfers (e.g., ``User views their current portfolio'', ``User selects an asset to send or receive'') rather than on the actual transfer and confirmation, which are the focuses of other models.}
	
	\item \textcolor{red}{Table (d) details the similarity for activity P4, showing a lower similarity among models with an average score of 2.6 in developing domain class diagrams from use case specifications in P3. ChatGPT exhibits the highest similarity with a score of 3, while Gemini records the lowest at 2. ChatGPT's higher similarity stems from its comprehensive coverage of core domain concepts such as $Wallet$, $Asset$, and $SmartContract$, which are also included by other models. In contrast, Gemini's lower score is due to its identification of only five domain concepts ($User$, $Wallet$, $Transaction$, $Currency$, $Amount$), missing many core concepts such as $SmartContract$, $SecuritySettings$, and $dApp$.}
	
	\item \textcolor{red}{Table (e) presents the similarity for activity P5, showing the lowest similarity among all activities with an average score of 1.8 in identifying system operations from the ``Manage Asset'' use case. Claude has the highest similarity with a score of 2.25, while ChatGPT has the lowest at 1.5. Claude's high similarity stems from its comprehensive and detailed operations in the use case, which encompass most operations covered by other models. Conversely, ChatGPT's low score is attributed to its limited focus on preparatory operations for fund transfer rather than on the actual transfer and confirmation operations, as previously discussed in Table (c).}
	
	\item \textcolor{red}{Table (f) reveals the similarity for activity P6, showing moderate similarity with small variations, averaging a score of 3 in developing the ``Transfer Assets'' sequence diagram. Copilot and Meta have relatively higher scores at 3.25, while ChatGPT and Claude have lower scores at 2.75. The lower scores for ChatGPT and Claude are mainly due to their inclusion of various practical participants (e.g., \emph{WalletService}, \emph{TransactionService}, \emph{BlockchainNetwork}) in their sequence diagrams, which are not covered by other models. Conversely, Gemini and Meta include only two participants ($User$ and $System$), misunderstanding the distinction between design sequence diagrams and system sequence diagrams, which typically feature only $User$ and $System$. Although having fewer participants makes their diagrams more similar to those of other models, it does not necessarily validate their output.}
	
	\item \textcolor{red}{Table (g) presents the similarity for activity P7, showing a low similarity among models with an average score of 2.2 in developing design class diagrams, based on the domain class diagrams from P4 and the design sequence diagrams from P6. ChatGPT exhibits the highest similarity with a score of 2.75, due to its comprehensive inclusion of core classes. Conversely, Gemini records the lowest similarity at 1.5, attributed to its limited coverage of classes, involving only five classes including non-critical ones such as \emph{Amount}, which is not included in other models' diagrams.}
	
	\item \textcolor{red}{Table (h) details the similarity for activity P8, showing a low similarity among models with an average score of 2 in implementing the ``Transaction'' class, which is common to all models. Meta exhibits the highest similarity with a score of 2.25, due to its inclusion of common core properties such as {\em transactionId}, {\em walletId}, and {\em assetId} for attributes, along with operations like {\em confirmTransaction()} and {\em cancelTransaction()}. On the other hand, Copilot's low similarity is attributed to its unique inclusion of properties such as {\em TransactionRules}, {\em configure()}, and {\em monitor()}, which are not featured in the other models.}
	
	\item \textcolor{red}{Lastly, Table (i) for P9 demonstrates a consistently low similarity across models with an average score of 2.2 in developing system tests for the ``Transfer Assets'' scenario. This consistent low similarity is attributed to the unique ways each model processes fund transfers. For example, Claude follows the sequence: [1.\emph{create\-Wallet} $\rightarrow$ 2.\emph{create\-Transaction} $\rightarrow$ 3.\emph{sign\-Transaction} $\rightarrow$ 4.\emph{broadcast\-Transaction} $\rightarrow$ 5.\emph{get\-Transaction\-Status} $\rightarrow$ 6.\emph{set\-Current\-Address} $\rightarrow$ 7.\emph{get\-Current\-Address}], while Meta has the sequence: [1.\emph{create\-Wallet} $\rightarrow$ 2.\emph{transfer assets} $\rightarrow$ 3.\emph{execute a smart contract} $\rightarrow$ 4.\emph{confirm transaction}]. These divergent sequences illustrate the distinctive strategies each model uses to apply its domain knowledge.}
\end{itemize}